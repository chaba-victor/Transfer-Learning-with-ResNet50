{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transfer Learning.\nIt is the process of reusing a pre-trained model on a new problem, rather than building a new model from scratch. The pre-trained model is usually trained on a large dataset for a specific task, such as image classification, speech recognition, or natural language processing (NLP). The knowledge and expertise gained from this task can then be applied to a new, related task, to improve its performance.\n\nIn this notebook, we'll take the follwoing steps.\n1. Importing libs.\n2. Reading data and splitting train and test.\n3. ResNet50 model Transfer Learning.\n4. Fine tuning Transfer Learning model.\n","metadata":{}},{"cell_type":"markdown","source":"# 1. Importing the necessary libraries.\n","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport numpy as np\nimport glob # a function that is used to search for files that match a specific file pattern or name  \nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, optimizers\nfrom tensorflow.keras.layers import Input, Add,Dropout, Dense, Activation, ZeroPadding2D, \\\nBatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.utils import  plot_model\nfrom tensorflow.keras.applications.imagenet_utils import preprocess_input\nfrom tensorflow.keras.initializers import glorot_uniform\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nfrom datetime import datetime \n\nfrom IPython.display import SVG\nimport scipy.misc\nfrom matplotlib.pyplot import imshow\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Reading data and splitting train and test.\n","metadata":{}},{"cell_type":"code","source":"# Where all dataset is there\ndata_dir = '/kaggle/input/flowers-recognition/flowers/'\n\n# Training data dir\ntraining_dir = '/kaggle/working/Train'\n\n# Test data dir\ntesting_dir = '/kaggle/working/Test'\n\n# Ratio of training and testing data\ntrain_test_ratio = 0.8 \n\ndef split_dataset_into_test_and_train_sets(all_data_dir = data_dir, training_data_dir = training_dir, \\\n                                           testing_data_dir=testing_dir, train_test_ratio = 0.8):\n    # Recreate testing and training directories\n    \n    if not os.path.exists(training_data_dir):\n            os.mkdir(training_data_dir)\n\n    if not os.path.exists(testing_data_dir):\n            os.mkdir(testing_data_dir)               \n    \n    num_training_files = 0\n    num_testing_files = 0\n\n\n    for subdir, dirs, files in os.walk(all_data_dir):\n        \n        category_name = os.path.basename(subdir)\n        \n        # print(category_name + \" vs \" + os.path.basename(all_data_dir))\n        if category_name == os.path.basename(all_data_dir):\n              continue\n\n        training_data_category_dir = training_data_dir + '/' + category_name\n        testing_data_category_dir = testing_data_dir + '/' + category_name\n        \n        # creating subdir for each sub category\n        if not os.path.exists(training_data_category_dir):\n            os.mkdir(training_data_category_dir)   \n\n        if not os.path.exists(testing_data_category_dir):\n            os.mkdir(testing_data_category_dir)\n            \n        file_list = glob.glob(os.path.join(subdir,'*.jpg'))\n\n        #print(os.path.join(all_data_dir, subdir))\n        print(str(category_name) + ' has ' + str(len(files)) + ' images') \n        random_set = np.random.permutation((file_list))\n        # copy percentage of data from each category to train and test directory\n        train_list = random_set[:round(len(random_set)*(train_test_ratio))] \n        test_list = random_set[-round(len(random_set)*(1-train_test_ratio)):]\n\n  \n\n        for lists in train_list : \n            shutil.copy(lists, training_data_dir + '/' + category_name + '/' )\n            num_training_files += 1\n  \n        for lists in test_list : \n            shutil.copy(lists, testing_data_dir + '/' + category_name + '/' )\n            num_testing_files += 1\n  \n\n    print(\"Processed \" + str(num_training_files) + \" training files.\")\n    print(\"Processed \" + str(num_testing_files) + \" testing files.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split_dataset_into_test_and_train_sets()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. ResNet50 model Transfer Learning.\n","metadata":{}},{"cell_type":"code","source":"# Number of classes in dataset\nnum_classes = 5\n\ndef get_model():\n    # Get base model \n    # Here we are using ResNet50 as base model\n    base_model = ResNet50(weights='imagenet', include_top=False)\n    \n    # As we are using ResNet model only for feature extraction and not adjusting the weights\n    # we freeze the layers in base model\n    for layer in base_model.layers:\n        layer.trainable = False\n        \n    # Get base model output \n    base_model_ouput = base_model.output\n    \n    # Adding our own layer \n    x = GlobalAveragePooling2D()(base_model_ouput)\n    # Adding fully connected layer\n    x = Dense(512, activation='relu')(x)\n    x = Dense(num_classes, activation='softmax', name='fcnew')(x)\n    \n    model = Model(inputs=base_model.input, outputs=x)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the model\nmodel = get_model()\n# Compile it\nmodel.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n# Summary of model\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining the imagedatagenerator for train and test image for pre-processing\n# We don't give horizonal_flip or other preprocessing for validation data generator\n\nimage_size = 224\nbatch_size = 64\n\ntrain_data_gen = ImageDataGenerator(preprocessing_function = preprocess_input,\n    shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\nvalid_data_gen = ImageDataGenerator(preprocessing_function = preprocess_input)\ntrain_generator = train_data_gen.flow_from_directory(training_dir, (image_size,image_size), batch_size=batch_size, class_mode='categorical')\nvalid_generator = valid_data_gen.flow_from_directory(testing_dir, (image_size,image_size), batch_size=batch_size, class_mode='categorical')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the fully conncected layer for initial epochs\nepochs = 5\n\n# Training the model\n\nmodel.fit(\n    train_generator,\n    steps_per_epoch=train_generator.n//batch_size,\n    validation_data=valid_generator,\n    validation_steps=valid_generator.n//batch_size,\n    epochs=epochs,\n    verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Fine tuning Transfer Learning model.","metadata":{}},{"cell_type":"code","source":"# More fine tuning the model\n# Training the model after 150 layers\n# Generally ResNet is good at extracting lower level features so we are not fine tuning initial layers\nepochs = 10\n\nsplit_at = 140\nfor layer in model.layers[:split_at]: layer.trainable = False\nfor layer in model.layers[split_at:]: layer.trainable = True\n    \nmodel.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Choosing lower learning rate for fine-tuning\n# learning rate is generally 10-1000 times lower than normal learning rate, if we are fine tuning the initial layers\nsgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\nmodel.fit(\n    train_generator,\n    steps_per_epoch=train_generator.n//batch_size,\n    validation_data=valid_generator,\n    validation_steps=valid_generator.n//batch_size,\n    epochs=epochs,\n    verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}